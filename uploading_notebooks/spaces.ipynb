{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 25 Feb 2024 23:10:34 GMT', 'content-length': '318', 'content-type': 'text/xml; charset=utf-8', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-envoy-upstream-healthchecked-cluster': ''}, 'RetryAttempts': 0}, 'Buckets': [{'Name': 'brbus-dataset', 'CreationDate': datetime.datetime(2024, 2, 20, 12, 47, 8, 928000, tzinfo=tzutc())}], 'Owner': {'DisplayName': '15685762', 'ID': '15685762'}}\n",
      "Spaces List: ['brbus-dataset']\n",
      "[{'Name': 'brbus-dataset', 'CreationDate': datetime.datetime(2024, 2, 20, 12, 47, 8, 928000, tzinfo=tzutc())}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "SPACES_KEY = os.getenv(\"SPACES_KEY\")\n",
    "SPACES_SECRET = os.getenv(\"SPACES_SECRET\")\n",
    "\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "client = session.client('s3',\n",
    "                        region_name='nyc3',\n",
    "                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n",
    "                        aws_access_key_id=SPACES_KEY,\n",
    "                        aws_secret_access_key=SPACES_SECRET)\n",
    "\n",
    "\n",
    "response = client.list_buckets()\n",
    "print(response)\n",
    "\n",
    "spaces = [space['Name'] for space in response['Buckets']]\n",
    "print(\"Spaces List: %s\" % spaces)\n",
    "\n",
    "print(response[\"Buckets\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "SPACES_KEY = os.getenv(\"SPACES_KEY\")\n",
    "SPACES_SECRET = os.getenv(\"SPACES_SECRET\")\n",
    "\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "client = session.client('s3',\n",
    "                        region_name='nyc3',\n",
    "                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n",
    "                        aws_access_key_id=SPACES_KEY,\n",
    "                        aws_secret_access_key=SPACES_SECRET)\n",
    "\n",
    "\n",
    "\n",
    "# files_list = []\n",
    "for city in [\"rj\",\"sp\",\"df\",\"cwb\"]:\n",
    "    path = f\"/home/felipe/dados_vm/{city}\"\n",
    "    for number,filename in enumerate(os.listdir(path)):\n",
    "        # files_list.append(filename)\n",
    "        client.upload_file(f\"/home/felipe/dados_vm/{city}/{filename}\",'brbus-dataset',f'bronze/{city}/{filename}')\n",
    "        print(number)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from delta import *\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Salvar Delta Table no Spaces\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.jars\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"DO00LNFAEQRLTA82CJCM\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"cOWHXzPEeesRVZSJDp/maav9CcnqYyyaOFh2HNWHcJ0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"nyc3.digitaloceanspaces.com\")\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "\n",
    "INPUT_PATH = \"/home/felipe/code/topicos_dados/dados/\"\n",
    "BRONZE_PATH = \"/home/felipe/code/deltalake/lake/bronze/\"\n",
    "SILVER_PATH = \"/home/felipe/code/deltalake/lake/silver/\"\n",
    "\n",
    "\n",
    "sp = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/sp_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "# sp.write.mode(\"overwrite\").format(\"delta\").save(\"/home/felipe/code/topicos_dados/lake/bronze/sp/\")\n",
    "sp.write.mode(\"overwrite\").format(\"delta\").option(\"path\",\"s3a://brbus-dataset/bronze/sp/\").saveAsTable(\"bronze_sp2\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from delta import *\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "\n",
    "builder = SparkSession.builder.appName(\"spark\") \\\n",
    ".config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    ".config(\"spark.jars.packages\",\"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\",\"DO00LNFAEQRLTA82CJCM\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\",\"cOWHXzPEeesRVZSJDp/maav9CcnqYyyaOFh2HNWHcJ0\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", \"nyc3.digitaloceanspaces.com\") \\\n",
    "\n",
    "builder = SparkSession.builder.appName(\"spark\").config(\"spark.hadoop.fs.s3a.access.key\",\"DO00LNFAEQRLTA82CJCM\").config(\"spark.hadoop.fs.s3a.secret.key\",\"cOWHXzPEeesRVZSJDp/maav9CcnqYyyaOFh2HNWHcJ0\").config(\"spark.hadoop.fs.s3a.endpoint\", \"nyc3.digitaloceanspaces.com\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "INPUT_PATH = \"/home/felipe/code/topicos_dados/dados/\"\n",
    "BRONZE_PATH = \"/home/felipe/code/deltalake/lake/bronze/\"\n",
    "SILVER_PATH = \"/home/felipe/code/deltalake/lake/silver/\"\n",
    "\n",
    "\n",
    "sp = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/sp_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "# sp.write.mode(\"overwrite\").format(\"delta\").save(\"/home/felipe/code/topicos_dados/lake/bronze/sp/\")\n",
    "sp.write.mode(\"overwrite\").format(\"delta\").option(\"path\",\"s3a://brbus-dataset/bronze/sp/\").saveAsTable(\"bronze_sp2\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from delta import *\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "\n",
    "builder = SparkSession.builder.appName(\"topicos\") \\\n",
    ".config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    ".config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.packages\",\"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4\").config(\"spark.hadoop.fs.s3a.access.key\",\"DO00LNFAEQRLTA82CJCM\").config(\"spark.hadoop.fs.s3a.secret.key\",\"cOWHXzPEeesRVZSJDp/maav9CcnqYyyaOFh2HNWHcJ0\").config(\"spark.hadoop.fs.s3a.endpoint\", \"nyc3.digitaloceanspaces.com\")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "INPUT_PATH = \"/home/felipe/code/topicos_dados/dados/\"\n",
    "BRONZE_PATH = \"/home/felipe/code/deltalake/lake/bronze/\"\n",
    "SILVER_PATH = \"/home/felipe/code/deltalake/lake/silver/\"\n",
    "\n",
    "\n",
    "sp = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/sp_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "# sp.write.mode(\"overwrite\").format(\"delta\").save(\"/home/felipe/code/topicos_dados/lake/bronze/sp/\")\n",
    "sp.write.mode(\"overwrite\").format(\"delta\").option(\"path\",\"s3a://brbus-dataset/bronze/sp/\").saveAsTable(\"bronze_sp2\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
