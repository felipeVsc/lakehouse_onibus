{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformações a serem realizadas\n",
    "\n",
    "- [ ] Remoção de arquivos com horário de ping diferente do horário de requsição.\n",
    "- [ ]  Ajustar hora em sp, diminuindo -3.\n",
    "- [ ]  Em Curitiba, quando o campo “codigolinha” estiver “REC”, o ônibus não está em operação, logo, será removido.\n",
    "- [ ] Ausência de valor no campo “linha” em BSB indica que não está em operação, logo deverá ser removido.\n",
    "- [ ] Atualizar campos de horas e datas para ISO 8601  2024-02-24T13:05Z.\n",
    "- [ ] Padronizar o sentido de operação da linha em SP e CWB para integers 1 = ida 2=  volta.\n",
    "- [ ] Padronizar os identificadores de ônibus CUR_idOnibus.\n",
    "- [ ] Add nome dos arquivos para um campo algo como \"query_timestamp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 20:28:35 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 192.168.0.106 instead (on interface enp6s0)\n",
      "24/03/11 20:28:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/felipe/.ivy2/cache\n",
      "The jars for the packages stored in: /home/felipe/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8c4de1a1-1fe3-48e9-b257-680e33928625;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/felipe/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 155ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8c4de1a1-1fe3-48e9-b257-680e33928625\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "24/03/11 20:28:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"topicos\").config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "INPUT_PATH = \"/home/felipe/code/topicos_dados/dados/\"\n",
    "BRONZE_PATH = \"/home/felipe/code/deltalake/lake/bronze/\"\n",
    "SILVER_PATH = \"/home/felipe/code/deltalake/lake/silver/\"\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+-------------------+--------------------+--------------------+---------+--------------------+\n",
      "|           latitude|          longitude|      c|                lt0|                 lt1|            query_at|   bus_id|          updated_at|\n",
      "+-------------------+-------------------+-------+-------------------+--------------------+--------------------+---------+--------------------+\n",
      "|         -23.552871|-46.647738000000004|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71212|2024-01-30T22:39:...|\n",
      "|        -23.6532175| -46.74036099999999|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71739|2024-01-30T22:38:...|\n",
      "|        -23.6680755|         -46.748797|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71361|2024-01-30T22:38:...|\n",
      "|         -23.534974|        -46.6443435|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71740|2024-01-30T22:38:...|\n",
      "|        -23.6168605| -46.70167549999999|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71839|2024-01-30T22:38:...|\n",
      "|      -23.574599125|        -46.6680825|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71844|2024-01-30T22:38:...|\n",
      "|-23.668001500000003|        -46.7491535|609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30T22:39:15Z|SPO_71860|2024-01-30T22:38:...|\n",
      "|         -23.541446|-46.536075499999995|407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30T22:39:15Z|SPO_48788|2024-01-30T22:39:...|\n",
      "|-23.549667499999998|-46.483762999999996|407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30T22:39:15Z|SPO_48985|2024-01-30T22:38:...|\n",
      "|        -23.5433355| -46.48539675000001|407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30T22:39:15Z|SPO_48445|2024-01-30T22:38:...|\n",
      "|        -23.5955305|           -46.6799|576M-10|          PINHEIROS|           VL. CLARA|2024-01-30T22:39:15Z|SPO_63370|2024-01-30T22:39:...|\n",
      "|        -23.6368035|         -46.641181|576M-10|          PINHEIROS|           VL. CLARA|2024-01-30T22:39:15Z|SPO_63366|2024-01-30T22:38:...|\n",
      "|-23.678067499999997|-46.640125499999996|576M-10|          PINHEIROS|           VL. CLARA|2024-01-30T22:39:15Z|SPO_63367|2024-01-30T22:39:...|\n",
      "|         -23.565959|        -46.6944145|576M-10|          PINHEIROS|           VL. CLARA|2024-01-30T22:39:15Z|SPO_63369|2024-01-30T22:38:...|\n",
      "|-23.678067499999997|-46.640125499999996|576M-10|          PINHEIROS|           VL. CLARA|2024-01-30T22:39:15Z|SPO_63344|2024-01-30T22:39:...|\n",
      "|         -23.570103|         -46.486686|3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30T22:39:15Z|SPO_45676|2024-01-30T22:38:...|\n",
      "|        -23.5753505|-46.497097499999995|3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30T22:39:15Z|SPO_45474|2024-01-30T22:38:...|\n",
      "|         -23.570103|         -46.486686|3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30T22:39:15Z|SPO_45530|2024-01-30T22:38:...|\n",
      "|-23.568366875000002|       -46.50955725|3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30T22:39:15Z|SPO_45327|2024-01-30T22:39:...|\n",
      "|       -23.67216275|        -46.6441475|5702-10|    METRÔ JABAQUARA|REFÚGIO STA. TERE...|2024-01-30T22:39:15Z|SPO_68258|2024-01-30T22:38:...|\n",
      "+-------------------+-------------------+-------+-------------------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# São Paulo\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Commmon Functions\n",
    "def changeBusIdSP(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"SPO_{onibus_id}\"\n",
    "\n",
    "def changeTimestamp(timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3\n",
    "    \"\"\"\n",
    "    datetime_object = datetime.fromisoformat(timestamp)\n",
    "    return (datetime_object-timedelta(hours=3)).isoformat()\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def checkInvalidTimeRows(df):\n",
    "    \n",
    "    df = df.withColumn(\"tempo_captura\", F.to_timestamp(\"tempo_captura\"))\n",
    "    df = df.withColumn(\"file_timestamp\", F.to_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    timediff = F.abs(F.unix_timestamp(\"tempo_captura\") - F.unix_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    return df.filter(timediff >= 300)\n",
    "\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdSpo = udf(changeBusIdSP,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestamp,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "\n",
    "# Reading DF\n",
    "sp = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/sp_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "# Changing DF\n",
    "sp = sp.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "sp = sp.withColumn(\"bus_id\",udf_transformBusIdSpo(col(\"id_onibus\")))\n",
    "sp = sp.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\")))\n",
    "\n",
    "# Droping columns\n",
    "sp = sp.drop(\"inputFiles\",\"id_onibus\",\"tempo_captura\")\n",
    "\n",
    "sp.show()\n",
    "\n",
    " # TODO add checkinvalidtimerows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+-----+----------+---------------+------+--------+--------------------+---------+-------------------+-------------+\n",
      "|  latitude| longitude|  tipo_veiculo|linha|  situacao|     situacao_2|tabela|adaptado|          queried_at|   bus_id|         updated_at|bus_direction|\n",
      "+----------+----------+--------------+-----+----------+---------------+------+--------+--------------------+---------+-------------------+-------------+\n",
      "|-25.453483| -49.28948|MICRO ESPECIAL|  762|NO HORÁRIO|REALIZANDO ROTA|   2-2|       1|2024-01-30T22:42:22Z|CWB_JI859|2024-01-3022:42:00Z|            2|\n",
      "|-25.481223|-49.196893|MICRO ESPECIAL|  463|NO HORÁRIO|REALIZANDO ROTA|     2|       1|2024-01-30T22:42:22Z|CWB_DN603|2024-01-3022:38:00Z|            1|\n",
      "|-25.432205|  -49.2658|MICRO ESPECIAL|  463|NO HORÁRIO|REALIZANDO ROTA|     4|       1|2024-01-30T22:42:22Z|CWB_DN608|2024-01-3022:42:00Z|            2|\n",
      "|-25.514096|-49.322986|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|     5|       1|2024-01-30T22:42:22Z|CWB_GB611|2024-01-3022:42:00Z|            2|\n",
      "|-25.516875|-49.318451|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|    12|       1|2024-01-30T22:42:22Z|CWB_HB609|2024-01-3022:42:00Z|            1|\n",
      "|-25.401065|-49.334415|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|    10|       1|2024-01-30T22:42:22Z|CWB_JB607|2024-01-3022:42:00Z|            2|\n",
      "|-25.441675| -49.34608|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|    17|       1|2024-01-30T22:42:22Z|CWB_JB608|2024-01-3022:42:00Z|            2|\n",
      "|-25.494071|-49.324508|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|   3-2|       1|2024-01-30T22:42:22Z|CWB_HB694|2024-01-3022:42:00Z|            1|\n",
      "|-25.477678|-49.332613|    ARTICULADO|  040|  ATRASADO|REALIZANDO ROTA|  22-2|       1|2024-01-30T22:42:22Z|CWB_HB606|2024-01-3022:42:00Z|            2|\n",
      "|-25.453843|-49.348961|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|   2-2|       1|2024-01-30T22:42:22Z|CWB_JB610|2024-01-3022:42:00Z|            1|\n",
      "|-25.419908|-49.348435|    ARTICULADO|  040|NO HORÁRIO|REALIZANDO ROTA|     6|       1|2024-01-30T22:42:22Z|CWB_JI603|2024-01-3022:42:00Z|            1|\n",
      "|-25.483838|-49.349955|        PADRON|  703|NO HORÁRIO|REALIZANDO ROTA|   7-2|       1|2024-01-30T22:42:22Z|CWB_JC313|2024-01-3022:42:00Z|            2|\n",
      "|-25.437491|-49.261078|        PADRON|  507|NO HORÁRIO|REALIZANDO ROTA|     6|       0|2024-01-30T22:42:22Z|CWB_EL313|2024-01-3022:42:00Z|            0|\n",
      "|-25.524845| -49.24804|        PADRON|  507|NO HORÁRIO|REALIZANDO ROTA|    12|       1|2024-01-30T22:42:22Z|CWB_EL320|2024-01-3022:41:00Z|            0|\n",
      "|-25.453988|-49.258961|        PADRON|  507|NO HORÁRIO|REALIZANDO ROTA|   9-2|       1|2024-01-30T22:42:22Z|CWB_EL323|2024-01-3022:39:00Z|            0|\n",
      "|-25.511368|-49.294335|        PADRON|  507|NO HORÁRIO|REALIZANDO ROTA|   1-2|       1|2024-01-30T22:42:22Z|CWB_EL319|2024-01-3022:41:00Z|            0|\n",
      "| -25.43062|-49.281191|         COMUM|  870|NO HORÁRIO|REALIZANDO ROTA|     2|       1|2024-01-30T22:42:22Z|CWB_BC009|2024-01-3022:41:00Z|            1|\n",
      "|-25.430041|-49.350676|         COMUM|  870|  ATRASADO|REALIZANDO ROTA|     3|       1|2024-01-30T22:42:22Z|CWB_BA014|2024-01-3022:42:00Z|            2|\n",
      "|-25.499676|-49.347065|         COMUM|  658|NO HORÁRIO|REALIZANDO ROTA|     2|       1|2024-01-30T22:42:22Z|CWB_HI039|2024-01-3022:42:00Z|            1|\n",
      "|-25.494401|-49.299198|         COMUM|  658|NO HORÁRIO|REALIZANDO ROTA|     3|       1|2024-01-30T22:42:22Z|CWB_JI002|2024-01-3022:42:00Z|            2|\n",
      "+----------+----------+--------------+-----+----------+---------------+------+--------+--------------------+---------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Curitiba\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdCWB(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"CWB_{onibus_id}\"\n",
    "\n",
    "def changeTimestampCwb(time,timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Curitiba\n",
    "\n",
    "    This is the 'tempo_captura' field in Curitiba: \"22:40\"\n",
    "    2024-01-30T\n",
    "    \"\"\"\n",
    "    return f\"{timestamp[:10]}{time}:00Z\"\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def checkInvalidTimeRows(df):\n",
    "    \n",
    "    df = df.withColumn(\"tempo_captura\", F.to_timestamp(\"tempo_captura\"))\n",
    "    df = df.withColumn(\"file_timestamp\", F.to_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    timediff = F.abs(F.unix_timestamp(\"tempo_captura\") - F.unix_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    return df.filter(timediff >= 300)\n",
    "\n",
    "def changeSentidoField(sentido):\n",
    "    sentidoMap = {\n",
    "        'IDA': 1,\n",
    "        'VOLTA': 2\n",
    "    }\n",
    "\n",
    "    return sentidoMap[sentido] if sentido in list(sentidoMap.keys()) else 0\n",
    "\n",
    "def removeInactiveBus(df):\n",
    "\n",
    "    filtered_df = df.filter(df['linha']!=\"REC\")\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdCwb = udf(changeBusIdCWB,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampCwb,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "udf_changeSentido = udf(changeSentidoField,IntegerType())\n",
    "\n",
    "# Reading DF\n",
    "\n",
    "cwb = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/cb_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "# Transformation\n",
    "\n",
    "cwb = cwb.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "cwb = cwb.withColumn(\"bus_id\",udf_transformBusIdCwb(col(\"id_onibus\")))\n",
    "cwb = cwb.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\"),col('queried_at')))\n",
    "cwb = cwb.withColumn(\"bus_direction\",udf_changeSentido(col(\"sentido\")))\n",
    "\n",
    "cwb = removeInactiveBus(cwb)\n",
    "\n",
    "# Dropping\n",
    "\n",
    "cwb = cwb.drop(\"tempo_captura\",\"sentido\",\"inputFiles\",\"id_onibus\")\n",
    "\n",
    "cwb.show()\n",
    "\n",
    "# TODO add check invalid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+-----+--------------------+--------------------+----------+-------------+\n",
      "|longitude| latitude|velocidade|   direcao|linha|          queried_at|          updated_at|    bus_id|bus_direction|\n",
      "+---------+---------+----------+----------+-----+--------------------+--------------------+----------+-------------+\n",
      "|-47.90188|-15.78581|     16.67|148.414433|0.882|2024-01-30T23:20:29Z|2024-01-301706665...|BSB_336921|            2|\n",
      "|-47.95254| -15.8094|     18.61| 166.02643|0.813|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_336408|            2|\n",
      "|-47.96208|-15.86806|      6.67|200.556045|084.1|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_336971|            1|\n",
      "|-48.14928|-15.89348|     13.89|100.479616|0.373|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_340065|            2|\n",
      "|-47.89276| -15.8111|      6.94| 22.280791|0.373|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_340103|            1|\n",
      "|-48.04888|-15.84341|       7.5|208.338534|0.805|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_336823|            1|\n",
      "|-48.06729|-15.80301|      8.33|23.9544515|0.053|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_335746|            1|\n",
      "|-48.08475|-15.86778|     11.11|221.193587| 3914|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_339920|            2|\n",
      "|-48.05605|-15.91339|      5.83|94.2363948|870.7|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_338940|            1|\n",
      "|-48.06893|-15.80939|       0.0|108.288026|372.4|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_336041|            1|\n",
      "|-47.95572|-15.85341|      6.39|257.828515|0.825|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_340880|            2|\n",
      "|-47.91467|-15.81844|     16.11|219.086183|813.2|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_337722|            2|\n",
      "|-48.06445|-15.90984|      5.28|285.242976|807.6|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_338974|            1|\n",
      "|-47.95509|-15.85028|     14.72|264.497394|813.2|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_337692|            2|\n",
      "|-47.90952|-15.82422|      15.0|36.5305892|0.172|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_337498|            1|\n",
      "|-48.10755|-15.82078|     12.22|106.105995|0.399|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_340171|            1|\n",
      "|-48.08121|-15.88157|     13.06|198.008722|366.1|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_340944|            2|\n",
      "|-48.08121|-15.88157|      2.78| 186.30444|396.3|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_335827|            2|\n",
      "|-48.06237|-15.90865|      6.94|4.44308727|874.4|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_339059|            2|\n",
      "|-48.10649|-15.92626|     10.56|231.060769|805.2|2024-01-30T23:20:29Z|2024-01-301706667...|BSB_338575|            1|\n",
      "+---------+---------+----------+----------+-----+--------------------+--------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brasilia\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdBSB(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"BSB_{onibus_id}\"\n",
    "\n",
    "def changeTimestampBsb(time,timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Curitiba\n",
    "\n",
    "    This is the 'tempo_captura' field in Curitiba: \"1701355514000\"\n",
    "    But the value isn't the same as the timestamp from querying.\n",
    "    \"\"\"\n",
    "    return f\"{timestamp[:10]}{time}:00Z\"\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def checkInvalidTimeRows(df):\n",
    "    \n",
    "    df = df.withColumn(\"tempo_captura\", F.to_timestamp(\"tempo_captura\"))\n",
    "    df = df.withColumn(\"file_timestamp\", F.to_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    timediff = F.abs(F.unix_timestamp(\"tempo_captura\") - F.unix_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    return df.filter(timediff >= 300)\n",
    "\n",
    "def changeSentidoField(sentido):\n",
    "    sentidoMap = {\n",
    "        'IDA': 1,\n",
    "        'VOLTA': 2\n",
    "    }\n",
    "\n",
    "    return sentidoMap[sentido] if sentido in list(sentidoMap.keys()) else 0\n",
    "\n",
    "def removeInactiveBus(df):\n",
    "\n",
    "    filtered_df = df.filter(df['linha']!=\"\")\n",
    "    return filtered_df\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdBSB = udf(changeBusIdBSB,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampBsb,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "udf_changeSentido = udf(changeSentidoField,IntegerType())\n",
    "\n",
    "bsb = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/df_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "bsb = bsb.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "bsb = bsb.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\"),col(\"queried_at\")))\n",
    "bsb = bsb.withColumn(\"bus_id\",udf_transformBusIdBSB(col(\"id_onibus\")))\n",
    "bsb = bsb.withColumn(\"bus_direction\",udf_changeSentido(col(\"sentido\")))\n",
    "\n",
    "bsb = removeInactiveBus(bsb)\n",
    "\n",
    "# dropping \n",
    "\n",
    "bsb = bsb.drop(\"tempo_captura\",\"sentido\",\"inputFiles\",\"id_onibus\")\n",
    "\n",
    "bsb.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+-----+--------------------+--------------------+----------+\n",
      "| latitude|longitude|velocidade|linha|          queried_at|          updated_at|    bus_id|\n",
      "+---------+---------+----------+-----+--------------------+--------------------+----------+\n",
      "|-22,91606|-43,23063|        37|  711|2024-01-31T22:43:00Z|2024-01-31T22:41:46Z|RJO_A29134|\n",
      "|-22,97236|-43,18777|        35|  473|2024-01-31T22:43:00Z|2024-01-31T22:41:49Z|RJO_A29108|\n",
      "|-22,89427|-43,19073|        38|  350|2024-01-31T22:43:00Z|2024-01-31T22:41:45Z|RJO_A29185|\n",
      "|-22,84925|-43,33784|        29|  711|2024-01-31T22:43:00Z|2024-01-31T22:41:48Z|RJO_A29034|\n",
      "|-22,89272|-43,21592|        37|  209|2024-01-31T22:43:00Z|2024-01-31T22:41:45Z|RJO_A29182|\n",
      "|-22,91143|-43,21638|        25|  457|2024-01-31T22:43:00Z|2024-01-31T22:41:50Z|RJO_A29192|\n",
      "|-22,89258|-43,23858|         9|  472|2024-01-31T22:43:00Z|2024-01-31T22:41:49Z|RJO_A29018|\n",
      "|-22,84079|-43,30468|        27|  350|2024-01-31T22:43:00Z|2024-01-31T22:41:51Z|RJO_A29191|\n",
      "|-22,97921|-43,23046|        27|  583|2024-01-31T22:43:00Z|2024-01-31T22:41:52Z|RJO_A29173|\n",
      "|-22,89927|-43,24474|        27|  476|2024-01-31T22:43:00Z|2024-01-31T22:41:52Z|RJO_A29092|\n",
      "|-22,91566|-43,20991|         0|  711|2024-01-31T22:43:00Z|2024-01-31T22:41:52Z|RJO_A29027|\n",
      "|-22,87756|-43,21615|         0|  209|2024-01-31T22:43:00Z|2024-01-31T22:41:53Z|RJO_A29180|\n",
      "| -22,8943|-43,23895|         1|  711|2024-01-31T22:43:00Z|2024-01-31T22:41:53Z|RJO_A29184|\n",
      "|-22,84932|-43,34375|        55|  711|2024-01-31T22:43:00Z|2024-01-31T22:41:53Z|RJO_A29094|\n",
      "|-22,96233|-43,16627|         0|  472|2024-01-31T22:43:00Z|2024-01-31T22:41:49Z|RJO_A29176|\n",
      "|-22,87685|-43,21877|        35|  209|2024-01-31T22:43:00Z|2024-01-31T22:41:51Z|RJO_A29155|\n",
      "|-22,86785|-43,29109|         0|  472|2024-01-31T22:43:00Z|2024-01-31T22:41:54Z|RJO_A29023|\n",
      "|-22,91906|-43,26175|        31|  435|2024-01-31T22:43:00Z|2024-01-31T22:41:50Z|RJO_A29186|\n",
      "| -22,9704|-43,18598|         0|  435|2024-01-31T22:43:00Z|2024-01-31T22:41:53Z|RJO_A29165|\n",
      "|-22,96472|-43,17804|        11|  473|2024-01-31T22:43:00Z|2024-01-31T22:41:53Z|RJO_A29162|\n",
      "+---------+---------+----------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rio de Janeiro\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdRj(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"RJO_{onibus_id}\"\n",
    "\n",
    "def changeTimestampRj(timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Rio de janeiro\n",
    "\n",
    "    This is the 'tempo_captura' field in Rio de Janeiro: \"1701355514000\"\n",
    "    \n",
    "    \"\"\"\n",
    "    time = (datetime.fromtimestamp(float(timestamp[:-3]))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def checkInvalidTimeRows(df):\n",
    "    \n",
    "    df = df.withColumn(\"tempo_captura\", F.to_timestamp(\"tempo_captura\"))\n",
    "    df = df.withColumn(\"file_timestamp\", F.to_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    timediff = F.abs(F.unix_timestamp(\"tempo_captura\") - F.unix_timestamp(\"file_timestamp\"))\n",
    "\n",
    "    return df.filter(timediff >= 300)\n",
    "\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdRj = udf(changeBusIdRj,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampRj,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "\n",
    "\n",
    "rj = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/rj_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "rj = rj.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "rj = rj.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\")))\n",
    "rj = rj.withColumn(\"bus_id\",udf_transformBusIdRj(col(\"id_onibus\")))\n",
    "\n",
    "# dropping\n",
    "\n",
    "rj = rj.drop(\"tempo_captura\",\"id_onibus\",\"inputFiles\")\n",
    "\n",
    "rj.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
