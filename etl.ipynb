{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformações a serem realizadas\n",
    "\n",
    "- [ ] Remoção de arquivos com horário de ping diferente do horário de requsição.\n",
    "- [ ]  Ajustar hora em sp, diminuindo -3.\n",
    "- [ ]  Em Curitiba, quando o campo “codigolinha” estiver “REC”, o ônibus não está em operação, logo, será removido.\n",
    "- [ ] Ausência de valor no campo “linha” em BSB indica que não está em operação, logo deverá ser removido.\n",
    "- [ ] Atualizar campos de horas e datas para ISO 8601  2024-02-24T13:05Z.\n",
    "- [ ] Padronizar o sentido de operação da linha em SP e CWB para integers 1 = ida 2=  volta.\n",
    "- [ ] Padronizar os identificadores de ônibus CUR_idOnibus.\n",
    "- [ ] Add nome dos arquivos para um campo algo como \"query_timestamp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 13:54:06 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 192.168.0.106 instead (on interface enp6s0)\n",
      "24/03/18 13:54:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/felipe/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/felipe/.ivy2/cache\n",
      "The jars for the packages stored in: /home/felipe/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-caab55e4-d394-49e3-828f-8132d27be247;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 1031ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-caab55e4-d394-49e3-828f-8132d27be247\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/10ms)\n",
      "24/03/18 13:54:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"topicos\").config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "INPUT_PATH = \"/home/felipe/code/topicos_dados/dados/\"\n",
    "BRONZE_PATH = \"/home/felipe/code/topicos_dados/lake/bronze/\"\n",
    "SILVER_PATH = \"/home/felipe/code/topicos_dados/lake/silver/\"\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def checkInvalidTimeRows(df):\n",
    "    \n",
    "    df = df.withColumn(\"updated_at\", F.to_timestamp(\"updated_at\"))\n",
    "    df = df.withColumn(\"queried_at\", F.to_timestamp(\"queried_at\"))\n",
    "\n",
    "    timediff = F.abs(F.unix_timestamp(\"updated_at\") - F.unix_timestamp(\"queried_at\"))\n",
    "\n",
    "    return df.filter(timediff < 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 14:48:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 14:48:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 14:48:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 14:48:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 14:48:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+-------------------+--------------------+-------------------+---------+-------------------+\n",
      "|           latitude|          longitude|bus_code|           station0|            station1|         queried_at|   bus_id|         updated_at|\n",
      "+-------------------+-------------------+--------+-------------------+--------------------+-------------------+---------+-------------------+\n",
      "|         -23.552871|-46.647738000000004| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71212|2024-01-30 19:39:09|\n",
      "|        -23.6532175| -46.74036099999999| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71739|2024-01-30 19:38:46|\n",
      "|        -23.6680755|         -46.748797| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71361|2024-01-30 19:38:37|\n",
      "|         -23.534974|        -46.6443435| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71740|2024-01-30 19:38:55|\n",
      "|        -23.6168605| -46.70167549999999| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71839|2024-01-30 19:38:36|\n",
      "|      -23.574599125|        -46.6680825| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71844|2024-01-30 19:38:47|\n",
      "|-23.668001500000003|        -46.7491535| 609F-10|TERM. PRINC. ISABEL|       CHÁC. SANTANA|2024-01-30 19:39:15|SPO_71860|2024-01-30 19:38:41|\n",
      "|         -23.541446|-46.536075499999995| 407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30 19:39:15|SPO_48788|2024-01-30 19:39:06|\n",
      "|-23.549667499999998|-46.483762999999996| 407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30 19:39:15|SPO_48985|2024-01-30 19:38:43|\n",
      "|        -23.5433355| -46.48539675000001| 407I-10|      METRÔ BRESSER|CONJ. MANOEL DA N...|2024-01-30 19:39:15|SPO_48445|2024-01-30 19:38:46|\n",
      "|        -23.5955305|           -46.6799| 576M-10|          PINHEIROS|           VL. CLARA|2024-01-30 19:39:15|SPO_63370|2024-01-30 19:39:01|\n",
      "|        -23.6368035|         -46.641181| 576M-10|          PINHEIROS|           VL. CLARA|2024-01-30 19:39:15|SPO_63366|2024-01-30 19:38:57|\n",
      "|-23.678067499999997|-46.640125499999996| 576M-10|          PINHEIROS|           VL. CLARA|2024-01-30 19:39:15|SPO_63367|2024-01-30 19:39:07|\n",
      "|         -23.565959|        -46.6944145| 576M-10|          PINHEIROS|           VL. CLARA|2024-01-30 19:39:15|SPO_63369|2024-01-30 19:38:32|\n",
      "|-23.678067499999997|-46.640125499999996| 576M-10|          PINHEIROS|           VL. CLARA|2024-01-30 19:39:15|SPO_63344|2024-01-30 19:39:08|\n",
      "|         -23.570103|         -46.486686| 3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30 19:39:15|SPO_45676|2024-01-30 19:38:36|\n",
      "|        -23.5753505|-46.497097499999995| 3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30 19:39:15|SPO_45474|2024-01-30 19:38:30|\n",
      "|         -23.570103|         -46.486686| 3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30 19:39:15|SPO_45530|2024-01-30 19:38:58|\n",
      "|-23.568366875000002|       -46.50955725| 3778-10|       METRÔ CARRÃO|  JD. STA. TEREZINHA|2024-01-30 19:39:15|SPO_45327|2024-01-30 19:39:13|\n",
      "|       -23.67216275|        -46.6441475| 5702-10|    METRÔ JABAQUARA|REFÚGIO STA. TERE...|2024-01-30 19:39:15|SPO_68258|2024-01-30 19:38:45|\n",
      "+-------------------+-------------------+--------+-------------------+--------------------+-------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# São Paulo\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Commmon Functions\n",
    "def changeBusIdSP(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"SPO_{onibus_id}\"\n",
    "\n",
    "def changeTimestamp(timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3\n",
    "    \"\"\"\n",
    "    datetime_object = datetime.fromisoformat(timestamp)\n",
    "    return str((datetime_object-timedelta(hours=3)).isoformat())\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdSpo = udf(changeBusIdSP,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestamp,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "\n",
    "# Reading DF\n",
    "sp = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/sp_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "# Changing DF\n",
    "sp = sp.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "sp = sp.withColumn(\"bus_id\",udf_transformBusIdSpo(col(\"id_onibus\")))\n",
    "sp = sp.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\")))\n",
    "\n",
    "# Droping columns\n",
    "sp = sp.drop(\"inputFiles\",\"id_onibus\",\"tempo_captura\")\n",
    "\n",
    "# Renaming columns\n",
    "\n",
    "sp = sp.withColumnsRenamed({\"lt0\":\"station0\",\"lt1\":\"station1\",\"c\":\"bus_code\"})\n",
    "\n",
    "\n",
    "# Testing filtering rows\n",
    "\n",
    "sp = checkInvalidTimeRows(sp)\n",
    "\n",
    "# saving\n",
    "sp.write.format(\"delta\").option(\"path\",f\"/home/felipe/code/topicos_dados/lake/silver/silver_sp\").saveAsTable(\"silver_sp\")\n",
    "\n",
    "sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS silver_sp\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_bsb\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_rj\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS silver_cwb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+--------+----------+---------------+-----+----------+-------------------+---------+-------------------+-------------+\n",
      "|  latitude| longitude|  type_vehicle|bus_code| situation|    situation_2|table|is_adapted|         queried_at|   bus_id|         updated_at|bus_direction|\n",
      "+----------+----------+--------------+--------+----------+---------------+-----+----------+-------------------+---------+-------------------+-------------+\n",
      "|-25.453483| -49.28948|MICRO ESPECIAL|     762|NO HORÁRIO|REALIZANDO ROTA|  2-2|         1|2024-01-30 19:42:22|CWB_JI859|2024-01-30 19:42:00|            2|\n",
      "|-25.481223|-49.196893|MICRO ESPECIAL|     463|NO HORÁRIO|REALIZANDO ROTA|    2|         1|2024-01-30 19:42:22|CWB_DN603|2024-01-30 19:38:00|            1|\n",
      "|-25.432205|  -49.2658|MICRO ESPECIAL|     463|NO HORÁRIO|REALIZANDO ROTA|    4|         1|2024-01-30 19:42:22|CWB_DN608|2024-01-30 19:42:00|            2|\n",
      "|-25.514096|-49.322986|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|    5|         1|2024-01-30 19:42:22|CWB_GB611|2024-01-30 19:42:00|            2|\n",
      "|-25.516875|-49.318451|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|   12|         1|2024-01-30 19:42:22|CWB_HB609|2024-01-30 19:42:00|            1|\n",
      "|-25.401065|-49.334415|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|   10|         1|2024-01-30 19:42:22|CWB_JB607|2024-01-30 19:42:00|            2|\n",
      "|-25.441675| -49.34608|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|   17|         1|2024-01-30 19:42:22|CWB_JB608|2024-01-30 19:42:00|            2|\n",
      "|-25.494071|-49.324508|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|  3-2|         1|2024-01-30 19:42:22|CWB_HB694|2024-01-30 19:42:00|            1|\n",
      "|-25.477678|-49.332613|    ARTICULADO|     040|  ATRASADO|REALIZANDO ROTA| 22-2|         1|2024-01-30 19:42:22|CWB_HB606|2024-01-30 19:42:00|            2|\n",
      "|-25.453843|-49.348961|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|  2-2|         1|2024-01-30 19:42:22|CWB_JB610|2024-01-30 19:42:00|            1|\n",
      "|-25.419908|-49.348435|    ARTICULADO|     040|NO HORÁRIO|REALIZANDO ROTA|    6|         1|2024-01-30 19:42:22|CWB_JI603|2024-01-30 19:42:00|            1|\n",
      "|-25.483838|-49.349955|        PADRON|     703|NO HORÁRIO|REALIZANDO ROTA|  7-2|         1|2024-01-30 19:42:22|CWB_JC313|2024-01-30 19:42:00|            2|\n",
      "|-25.437491|-49.261078|        PADRON|     507|NO HORÁRIO|REALIZANDO ROTA|    6|         0|2024-01-30 19:42:22|CWB_EL313|2024-01-30 19:42:00|            0|\n",
      "|-25.524845| -49.24804|        PADRON|     507|NO HORÁRIO|REALIZANDO ROTA|   12|         1|2024-01-30 19:42:22|CWB_EL320|2024-01-30 19:41:00|            0|\n",
      "|-25.453988|-49.258961|        PADRON|     507|NO HORÁRIO|REALIZANDO ROTA|  9-2|         1|2024-01-30 19:42:22|CWB_EL323|2024-01-30 19:39:00|            0|\n",
      "|-25.511368|-49.294335|        PADRON|     507|NO HORÁRIO|REALIZANDO ROTA|  1-2|         1|2024-01-30 19:42:22|CWB_EL319|2024-01-30 19:41:00|            0|\n",
      "| -25.43062|-49.281191|         COMUM|     870|NO HORÁRIO|REALIZANDO ROTA|    2|         1|2024-01-30 19:42:22|CWB_BC009|2024-01-30 19:41:00|            1|\n",
      "|-25.430041|-49.350676|         COMUM|     870|  ATRASADO|REALIZANDO ROTA|    3|         1|2024-01-30 19:42:22|CWB_BA014|2024-01-30 19:42:00|            2|\n",
      "|-25.499676|-49.347065|         COMUM|     658|NO HORÁRIO|REALIZANDO ROTA|    2|         1|2024-01-30 19:42:22|CWB_HI039|2024-01-30 19:42:00|            1|\n",
      "|-25.494401|-49.299198|         COMUM|     658|NO HORÁRIO|REALIZANDO ROTA|    3|         1|2024-01-30 19:42:22|CWB_JI002|2024-01-30 19:42:00|            2|\n",
      "+----------+----------+--------------+--------+----------+---------------+-----+----------+-------------------+---------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Curitiba\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdCWB(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"CWB_{onibus_id}\"\n",
    "\n",
    "def changeTimestampCwb(time,timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Curitiba\n",
    "\n",
    "    This is the 'tempo_captura' field in Curitiba: \"22:40\"\n",
    "    2024-01-30T\n",
    "    \"\"\"\n",
    "    return f\"{timestamp[:10]}T{time}:00Z\"\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def changeSentidoField(sentido):\n",
    "    sentidoMap = {\n",
    "        'IDA': 1,\n",
    "        'VOLTA': 2\n",
    "    }\n",
    "\n",
    "    return sentidoMap[sentido] if sentido in list(sentidoMap.keys()) else 0\n",
    "\n",
    "def removeInactiveBus(df):\n",
    "\n",
    "    filtered_df = df.filter(df['linha']!=\"REC\")\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdCwb = udf(changeBusIdCWB,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampCwb,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "udf_changeSentido = udf(changeSentidoField,IntegerType())\n",
    "\n",
    "# Reading DF\n",
    "\n",
    "cwb = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/cb_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "# Transformation\n",
    "\n",
    "cwb = cwb.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "cwb = cwb.withColumn(\"bus_id\",udf_transformBusIdCwb(col(\"id_onibus\")))\n",
    "cwb = cwb.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\"),col('queried_at')))\n",
    "cwb = cwb.withColumn(\"bus_direction\",udf_changeSentido(col(\"sentido\")))\n",
    "\n",
    "cwb = removeInactiveBus(cwb)\n",
    "\n",
    "# Dropping\n",
    "\n",
    "cwb = cwb.drop(\"tempo_captura\",\"sentido\",\"inputFiles\",\"id_onibus\")\n",
    "\n",
    "# Renamming\n",
    "\n",
    "cwb = cwb.withColumnsRenamed({\n",
    "    \"adaptado\":\"is_adapted\",\n",
    "    \"linha\":\"bus_code\",\n",
    "    \"tipo_veiculo\":\"type_vehicle\",\n",
    "    \"situacao\":\"situation\",\n",
    "    \"situacao_2\":\"situation_2\",\n",
    "    \"tabela\":\"table\"\n",
    "})\n",
    "\n",
    "cwb = checkInvalidTimeRows(cwb)\n",
    "\n",
    "# Casting types\n",
    "\n",
    "cwb = cwb.withColumn(\"latitude\", cwb[\"latitude\"].cast(DoubleType()))\n",
    "cwb = cwb.withColumn(\"longitude\", cwb[\"longitude\"].cast(DoubleType()))\n",
    "cwb = cwb.withColumn(\"is_adapted\", cwb[\"is_adapted\"].cast(IntegerType()))\n",
    "\n",
    "# Saving\n",
    "\n",
    "cwb.write.format(\"delta\").option(\"path\",f\"/home/felipe/code/topicos_dados/lake/silver/silver_cwb\").saveAsTable(\"silver_cwb\")\n",
    "\n",
    "cwb.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 16:19:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:13 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:13 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:13 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:13 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:13 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:14 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:14 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:14 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:14 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+----------+--------+-------------------+-------------------+----------+-------------+\n",
      "|longitude| latitude|bus_speed| direction|bus_code|         queried_at|         updated_at|    bus_id|bus_direction|\n",
      "+---------+---------+---------+----------+--------+-------------------+-------------------+----------+-------------+\n",
      "|-47.94006|-15.81103|    13.06|12.8996672|   391.3|2024-02-02 04:32:40|2024-02-02 04:27:41|BSB_339261|            1|\n",
      "|-47.89697|-15.84607|      0.0|17.2043209|   0.822|2024-02-02 04:32:40|2024-02-02 04:27:41|BSB_337005|            1|\n",
      "|-47.86511|-15.76293|    14.44|304.305342|   0.884|2024-02-02 04:32:40|2024-02-02 04:27:41|BSB_337072|            2|\n",
      "|-47.93695|-15.86751|    16.11| 253.43652|   0.835|2024-02-02 04:32:40|2024-02-02 04:27:42|BSB_335479|            1|\n",
      "|-48.06735|-15.87217|     4.72|15.0089136|   0.841|2024-02-02 04:32:40|2024-02-02 04:27:42|BSB_339351|            1|\n",
      "|-48.10607|-15.88023|      0.0|18.3611965|   367.2|2024-02-02 04:32:40|2024-02-02 04:27:42|BSB_340341|            1|\n",
      "|-48.10168|-15.89452|    21.39|181.889442|   391.3|2024-02-02 04:32:40|2024-02-02 04:27:43|BSB_340022|            2|\n",
      "|-47.92988|-15.80759|    13.61|12.1198033|   0.810|2024-02-02 04:32:40|2024-02-02 04:27:43|BSB_338842|            1|\n",
      "|-47.93581|-15.76959|     6.94|206.565051|   0.876|2024-02-02 04:32:40|2024-02-02 04:27:43|BSB_338010|            1|\n",
      "|-48.06225|-15.91692|    10.28|286.186865|   0.873|2024-02-02 04:32:40|2024-02-02 04:27:44|BSB_337595|            1|\n",
      "|-48.07552|-15.84451|    17.22| 280.55254|   0.399|2024-02-02 04:32:40|2024-02-02 04:27:46|BSB_340219|            2|\n",
      "|-47.89153|-15.76169|    14.44|117.400858|   0.175|2024-02-02 04:32:40|2024-02-02 04:27:46|BSB_339628|            1|\n",
      "|-47.92834|-15.80722|    17.78|359.672736|   0.882|2024-02-02 04:32:40|2024-02-02 04:27:46|BSB_337951|            1|\n",
      "| -48.0513|-15.92593|      2.5|110.556045|   870.7|2024-02-02 04:32:40|2024-02-02 04:27:47|BSB_337048|            1|\n",
      "|-47.88761|-15.78507|     12.5|58.9573027|   396.3|2024-02-02 04:32:40|2024-02-02 04:27:48|BSB_335452|            1|\n",
      "|-47.92434|-15.80488|     1.94|28.1785901|   0.810|2024-02-02 04:32:40|2024-02-02 04:27:48|BSB_337943|            1|\n",
      "| -48.0998|-15.81504|     9.72|26.4282052|   0.886|2024-02-02 04:32:40|2024-02-02 04:27:48|BSB_338443|            2|\n",
      "|-48.14754| -15.8988|     1.39|200.826564|   0.367|2024-02-02 04:32:40|2024-02-02 04:27:49|BSB_336653|            1|\n",
      "| -48.0725|-15.85158|      0.0|123.736756|   812.2|2024-02-02 04:32:40|2024-02-02 04:27:49|BSB_337871|            1|\n",
      "|-48.04247|-15.92801|      0.0|108.795116|   0.811|2024-02-02 04:32:40|2024-02-02 04:27:49|BSB_337820|            1|\n",
      "+---------+---------+---------+----------+--------+-------------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brasilia\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdBSB(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"BSB_{onibus_id}\"\n",
    "\n",
    "def changeTimestampBsb(timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Curitiba\n",
    "\n",
    "    This is the 'tempo_captura' field in Curitiba: \"1701355514000\"\n",
    "    But the value isn't the same as the timestamp from querying. \n",
    "    \"\"\"\n",
    "    time = (datetime.fromtimestamp(float(str(timestamp)[:10]))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time\n",
    "\n",
    "def changeSentidoField(sentido):\n",
    "    sentidoMap = {\n",
    "        'IDA': 1,\n",
    "        'VOLTA': 2\n",
    "    }\n",
    "\n",
    "    return sentidoMap[sentido] if sentido in list(sentidoMap.keys()) else 0\n",
    "\n",
    "def removeInactiveBus(df):\n",
    "\n",
    "    filtered_df = df.filter(df['linha']!=\"\")\n",
    "    return filtered_df\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdBSB = udf(changeBusIdBSB,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampBsb,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "udf_changeSentido = udf(changeSentidoField,IntegerType())\n",
    "\n",
    "bsb = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/df_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "\n",
    "bsb = bsb.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "bsb = bsb.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\")))\n",
    "bsb = bsb.withColumn(\"bus_id\",udf_transformBusIdBSB(col(\"id_onibus\")))\n",
    "bsb = bsb.withColumn(\"bus_direction\",udf_changeSentido(col(\"sentido\")))\n",
    "\n",
    "bsb = removeInactiveBus(bsb)\n",
    "bsb = checkInvalidTimeRows(bsb)\n",
    "\n",
    "# dropping \n",
    "\n",
    "bsb = bsb.drop(\"tempo_captura\",\"sentido\",\"inputFiles\",\"id_onibus\")\n",
    "\n",
    "# renaming\n",
    "\n",
    "bsb = bsb.withColumnsRenamed({\n",
    "    \"velocidade\":\"bus_speed\",\n",
    "    \"linha\":\"bus_code\",\n",
    "    \"direcao\":\"direction\"\n",
    "})\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "bsb.write.format(\"delta\").option(\"path\",f\"/home/felipe/code/topicos_dados/lake/silver/silver_bsb\").saveAsTable(\"silver_bsb\")\n",
    "\n",
    "bsb.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 16:19:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:36 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:36 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:36 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:36 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:37 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "24/03/18 16:19:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/03/18 16:19:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/03/18 16:19:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/03/18 16:19:58 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/03/18 16:19:58 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+--------+-------------------+-------------------+----------+\n",
      "| latitude|longitude|bus_speed|bus_code|         queried_at|         updated_at|    bus_id|\n",
      "+---------+---------+---------+--------+-------------------+-------------------+----------+\n",
      "|-22.94364|-43.25486|      0.0|     220|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_A50159|\n",
      "|-23.01045| -43.2975|      8.0|     302|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_C50118|\n",
      "|-22.93032|-43.23769|      0.0|     220|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_A50025|\n",
      "|-23.00146|-43.36449|     16.0|   SP805|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_C50047|\n",
      "|-22.92475|-43.25124|     27.0|     608|2024-02-02 08:03:00|2024-02-02 08:01:49|RJO_A50187|\n",
      "|-22.92565|-43.24485|     24.0|     608|2024-02-02 08:03:00|2024-02-02 08:01:46|RJO_A50127|\n",
      "|-22.92509|-43.23414|     18.0|     645|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_C50116|\n",
      "|-23.00145|-43.36455|     15.0|   SP805|2024-02-02 08:03:00|2024-02-02 08:01:49|RJO_C50047|\n",
      "|-22.90949|-43.20941|     28.0|     301|2024-02-02 08:03:00|2024-02-02 08:01:47|RJO_A50002|\n",
      "|-22.93596|-43.24446|     24.0|     301|2024-02-02 08:03:00|2024-02-02 08:01:48|RJO_C50100|\n",
      "|-23.00155|-43.36578|     11.0|   SP805|2024-02-02 08:03:00|2024-02-02 08:01:50|RJO_C50134|\n",
      "|-22.94949|-43.18718|      0.0|     157|2024-02-02 08:03:00|2024-02-02 08:01:52|RJO_A50023|\n",
      "|-22.92421|-43.25417|      0.0|     608|2024-02-02 08:03:00|2024-02-02 08:01:51|RJO_A50051|\n",
      "|-22.97943|-43.29247|     22.0|     302|2024-02-02 08:03:00|2024-02-02 08:01:51|RJO_C50087|\n",
      "|-22.94146|-43.25234|     20.0|     301|2024-02-02 08:03:00|2024-02-02 08:01:51|RJO_C50110|\n",
      "|-22.92293|-43.26445|      1.0|     608|2024-02-02 08:03:00|2024-02-02 08:01:52|RJO_C50048|\n",
      "|-23.01012|-43.35044|      7.0|     805|2024-02-02 08:03:00|2024-02-02 08:01:52|RJO_C50135|\n",
      "|-22.93229|-43.24081|     13.0|     220|2024-02-02 08:03:00|2024-02-02 08:01:52|RJO_A50125|\n",
      "|-23.00147|-43.36476|     21.0|   SP805|2024-02-02 08:03:00|2024-02-02 08:01:51|RJO_C50047|\n",
      "|-23.00173| -43.3656|      0.0|   SP805|2024-02-02 08:03:00|2024-02-02 08:01:52|RJO_C50014|\n",
      "+---------+---------+---------+--------+-------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rio de Janeiro\n",
    "\n",
    "from pyspark.sql.functions import udf, input_file_name, col\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def changeBusIdRj(onibus_id):\n",
    "    \"\"\"\n",
    "    Change the column \"id_onibus\" to the following pattern: CITY_id_onibus\n",
    "\n",
    "    Example: SP_0881\n",
    "    \"\"\"\n",
    "    return f\"RJO_{onibus_id}\"\n",
    "\n",
    "def changeTimestampRj(timestamp):\n",
    "    \"\"\" \n",
    "    Change timestamp to ISO 8601 (2024-02-24T13:05Z) using GMT-3 for Rio de janeiro\n",
    "\n",
    "    This is the 'tempo_captura' field in Rio de Janeiro: \"1701355514000\"\n",
    "    \n",
    "    \"\"\"\n",
    "    time = (datetime.fromtimestamp(float(timestamp[:-3]))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return time\n",
    "\n",
    "def addTimestampQueryTime(filenames):\n",
    "    \"\"\" \n",
    "    Add the column \"query_timestamp\" indicating the timestamp\n",
    "    \n",
    "    file:///home/felipe/code/topicos_dados/dados/cb_micro/1706665101.9679544.parquet -> 1706665101.9679544\n",
    "    \"\"\"\n",
    "    file_name = str(filenames)[54:-8]\n",
    "    time = (datetime.fromtimestamp(float(file_name))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return time\n",
    "\n",
    "def changeSeparatorString(values):\n",
    "    return values.replace(\",\",\".\")\n",
    "\n",
    "# UDFS\n",
    "\n",
    "udf_transformBusIdRj = udf(changeBusIdRj,StringType())\n",
    "udf_changeTimestamp = udf(changeTimestampRj,StringType())\n",
    "udf_addTimestampFile = udf(addTimestampQueryTime,StringType())\n",
    "udf_changeSeparator = udf(changeSeparatorString,StringType())\n",
    "\n",
    "rj = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"{INPUT_PATH}/rj_micro\").withColumn(\"inputFiles\",input_file_name())\n",
    "\n",
    "rj = rj.withColumn(\"queried_at\",udf_addTimestampFile(col(\"inputFiles\")))\n",
    "rj = rj.withColumn(\"updated_at\",udf_changeTimestamp(col(\"tempo_captura\")))\n",
    "rj = rj.withColumn(\"bus_id\",udf_transformBusIdRj(col(\"id_onibus\")))\n",
    "rj = rj.withColumn(\"latitude\",udf_changeSeparator(col(\"latitude\")))\n",
    "rj = rj.withColumn(\"longitude\",udf_changeSeparator(col(\"longitude\")))\n",
    "\n",
    "# dropping\n",
    "\n",
    "rj = rj.drop(\"tempo_captura\",\"id_onibus\",\"inputFiles\")\n",
    "\n",
    "# Renaming\n",
    "\n",
    "rj = rj.withColumnsRenamed({\n",
    "    \"velocidade\":\"bus_speed\",\n",
    "    \"linha\":\"bus_code\",\n",
    "})\n",
    "\n",
    "rj = checkInvalidTimeRows(rj)\n",
    "\n",
    "# casting\n",
    "\n",
    "rj = rj.withColumn(\"latitude\", rj[\"latitude\"].cast(DoubleType()))\n",
    "rj = rj.withColumn(\"longitude\", rj[\"longitude\"].cast(DoubleType()))\n",
    "rj = rj.withColumn(\"bus_speed\", rj[\"bus_speed\"].cast(DoubleType()))\n",
    "\n",
    "\n",
    "rj.write.format(\"delta\").option(\"path\",f\"/home/felipe/code/topicos_dados/lake/silver/silver_rj\").saveAsTable(\"silver_rj\")\n",
    "\n",
    "rj.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
